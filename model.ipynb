{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b094660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a022a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Configuration\n",
    "# ---------------------------\n",
    "CONFIG = {\n",
    "    \"CSV_PATH\": \"dataset/mango_price_dataset.csv\",\n",
    "    \"RANDOM_STATE\": 42,\n",
    "    \"TEST_SIZE\": 0.20,\n",
    "    \"OUT_ROOT\": \"output\",\n",
    "    \"MODEL_DIR\": os.path.join(\"output\", \"models\"),\n",
    "    \"PLOTS_DIR\": os.path.join(\"output\", \"artifacts\", \"plots\"),\n",
    "    \"ARTIFACT_DIR\": os.path.join(\"output\", \"artifacts\"),\n",
    "    # classifier/regressor hyperparameters (from your tuning; tweak if desired)\n",
    "    \"CAT_CLASS_PARAMS\": {\"depth\": 4, \"iterations\": 395, \"learning_rate\": 0.23419603304121425, \"l2_leaf_reg\": 8.600804638103362},\n",
    "    \"XGB_CLASS_PARAMS\": {\"n_estimators\": 329, \"max_depth\": 5, \"learning_rate\": 0.05603751468349913, \"subsample\": 0.8435991664891758, \"colsample_bytree\": 0.9750220237762934},\n",
    "    \"CAT_REG_PARAMS\": {\"depth\": 5, \"iterations\": 566, \"learning_rate\": 0.12481222299146678, \"l2_leaf_reg\": 1.8997742423620259},\n",
    "    \"XGB_REG_PARAMS\": {\"n_estimators\": 441, \"max_depth\": 3, \"learning_rate\": 0.21386535711370855, \"subsample\": 0.8916028672163949, \"colsample_bytree\": 0.602208846849441},\n",
    "    \"BEST_PETTAH_WEIGHT\": 1.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adeaa4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Ensure folders\n",
    "# ---------------------------\n",
    "for p in (CONFIG[\"OUT_ROOT\"], CONFIG[\"MODEL_DIR\"], CONFIG[\"PLOTS_DIR\"], CONFIG[\"ARTIFACT_DIR\"]):\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0bd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Feature engineering\n",
    "# ---------------------------\n",
    "def prepare_full_features(df):\n",
    "    \"\"\"Create time features, lag/rolling, momentum, engineered features and interactions.\"\"\"\n",
    "    d = df.copy()\n",
    "    d['Date'] = pd.to_datetime(d['Date'])\n",
    "    d = d.sort_values(['Region', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # time-based\n",
    "    d['day'] = d['Date'].dt.day\n",
    "    d['month'] = d['Date'].dt.month\n",
    "    d['dayofyear'] = d['Date'].dt.dayofyear\n",
    "    d['year'] = d['Date'].dt.year\n",
    "    d['month_sin'] = np.sin(2 * np.pi * d['month'] / 12)\n",
    "    d['month_cos'] = np.cos(2 * np.pi * d['month'] / 12)\n",
    "    d['doy_sin'] = np.sin(2 * np.pi * d['dayofyear'] / 365)\n",
    "    d['doy_cos'] = np.cos(2 * np.pi * d['dayofyear'] / 365)\n",
    "\n",
    "    # lags\n",
    "    d['local_price_lag1'] = d.groupby('Region')['Local_Price_LKR'].shift(1)\n",
    "    d['export_price_lag1'] = d.groupby('Region')['Export_Price_USD'].shift(1)\n",
    "\n",
    "    # rolling means\n",
    "    d['local_price_roll7'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(7, min_periods=1).mean())\n",
    "    d['local_price_roll14'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
    "    d['export_price_roll7'] = d.groupby('Region')['Export_Price_USD'].transform(lambda s: s.rolling(7, min_periods=1).mean())\n",
    "    d['export_price_roll14'] = d.groupby('Region')['Export_Price_USD'].transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
    "\n",
    "    # momentum\n",
    "    d['local_price_mom1'] = d['Local_Price_LKR'] - d['local_price_lag1']\n",
    "    d['export_price_mom1'] = d['Export_Price_USD'] - d['export_price_lag1']\n",
    "\n",
    "    # fills for lag/rolling\n",
    "    grp_med_local = d.groupby('Region')['Local_Price_LKR'].transform('median')\n",
    "    grp_med_export = d.groupby('Region')['Export_Price_USD'].transform('median')\n",
    "\n",
    "    for col in ['local_price_lag1','local_price_roll7','local_price_roll14']:\n",
    "        d[col] = d[col].fillna(grp_med_local).fillna(d['Local_Price_LKR'].median())\n",
    "    for col in ['export_price_lag1','export_price_roll7','export_price_roll14']:\n",
    "        d[col] = d[col].fillna(grp_med_export).fillna(d['Export_Price_USD'].median())\n",
    "    d['local_price_mom1'] = d['local_price_mom1'].fillna(0)\n",
    "    d['export_price_mom1'] = d['export_price_mom1'].fillna(0)\n",
    "\n",
    "    # new features\n",
    "    d['local_price_region_mean'] = d.groupby('Region')['Local_Price_LKR'].transform('mean')\n",
    "    d['price_dev'] = d['Local_Price_LKR'] - d['local_price_region_mean']\n",
    "    d['price_to_age_ratio'] = d['Local_Price_LKR'] / (d['Mango_Age_Days'].replace(0,1))\n",
    "    d['local_price_vol7'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(7, min_periods=1).std()).fillna(0)\n",
    "    d['local_price_vol14'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(14, min_periods=1).std()).fillna(0)\n",
    "    d['local_mom_region_mean'] = d.groupby('Region')['local_price_mom1'].transform('mean')\n",
    "    d['momentum_dev'] = d['local_price_mom1'] - d['local_mom_region_mean']\n",
    "\n",
    "    # interactions\n",
    "    d['dev_x_ratio'] = d['price_dev'] * d['price_to_age_ratio']\n",
    "    d['vol_x_mom'] = d['local_price_vol14'] * d['momentum_dev']\n",
    "    d['age_x_price'] = d['Mango_Age_Days'] * d['local_price_lag1']\n",
    "    d['roll14_x_mom'] = d['local_price_roll14'] * d['local_price_mom1']\n",
    "\n",
    "    # fill residual NaNs for engineered numeric columns\n",
    "    fill_cols = ['price_dev','price_to_age_ratio','local_price_vol7','local_price_vol14','momentum_dev','dev_x_ratio','vol_x_mom','age_x_price','roll14_x_mom']\n",
    "    for c in fill_cols:\n",
    "        if c in d.columns:\n",
    "            d[c] = d[c].fillna(0)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9ef805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Utilities: plotting & feature name extraction\n",
    "# ---------------------------\n",
    "def save_confusion_matrix(cm, labels, outpath, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "\n",
    "def save_feature_importance_catboost(cat_model, preprocessor, numeric_features, categorical_features, outpath_csv, outpath_png, top_n=20):\n",
    "    try:\n",
    "        imp = cat_model.get_feature_importance(type='FeatureImportance')\n",
    "    except Exception:\n",
    "        imp = getattr(cat_model, 'feature_importances_', None)\n",
    "    if imp is None:\n",
    "        print(\"Feature importances not available\")\n",
    "        return None\n",
    "\n",
    "    # get feature names from preprocessor\n",
    "    names = list(numeric_features)\n",
    "    ohe = preprocessor.named_transformers_['cat']\n",
    "    try:\n",
    "        ohe_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "    except Exception:\n",
    "        # fallback older sklearn\n",
    "        cats = ohe.categories_\n",
    "        ohe_names = []\n",
    "        for col, cat_list in zip(categorical_features, cats):\n",
    "            for cat in cat_list:\n",
    "                ohe_names.append(f\"{col}__{cat}\")\n",
    "    all_names = names + ohe_names\n",
    "    fi = pd.Series(imp, index=all_names).sort_values(ascending=False)\n",
    "    fi.head(top_n).to_csv(outpath_csv)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    fi.head(top_n).sort_values().plot.barh()\n",
    "    plt.title(\"Top feature importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath_png)\n",
    "    plt.close()\n",
    "    return fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f31af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Train & evaluate function\n",
    "# ---------------------------\n",
    "def train_and_evaluate(csv_path, config):\n",
    "    # load\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"Loaded:\", df.shape)\n",
    "\n",
    "    # prepare features\n",
    "    dfc = prepare_full_features(df)\n",
    "    print(\"Prepared features; columns:\", len(dfc.columns))\n",
    "\n",
    "    # features & targets\n",
    "    feature_cols = [\n",
    "        'month_sin','month_cos','doy_sin','doy_cos',\n",
    "        'Mango_Age_Days','Days_To_Maturity','Temp_C','Humidity_%',\n",
    "        'Region','weather',\n",
    "        'local_price_lag1','local_price_roll7','local_price_roll14','local_price_mom1',\n",
    "        'export_price_lag1','export_price_roll7','export_price_roll14','export_price_mom1',\n",
    "        'price_dev','price_to_age_ratio','local_price_vol7','local_price_vol14','momentum_dev',\n",
    "        'dev_x_ratio','vol_x_mom','age_x_price','roll14_x_mom'\n",
    "    ]\n",
    "\n",
    "    missing = set(feature_cols) - set(dfc.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\"Missing engineered features: \" + str(missing))\n",
    "\n",
    "    X = dfc[feature_cols].copy()\n",
    "    y_local = dfc['Local_Market'].values\n",
    "    y_export = dfc['Export_Market'].values\n",
    "    y_reg = dfc[[\"Local_Price_LKR\",\"Export_Price_USD\",\"Harvesting_after_3months_price\"]].values\n",
    "\n",
    "    # encoders\n",
    "    le_local = LabelEncoder(); y_local_enc = le_local.fit_transform(y_local)\n",
    "    le_export = LabelEncoder(); y_export_enc = le_export.fit_transform(y_export)\n",
    "\n",
    "    # train/test splits\n",
    "    X_train_cls, X_test_cls, y_local_train, y_local_test, y_export_train, y_export_test = train_test_split(\n",
    "        X, y_local_enc, y_export_enc, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"],\n",
    "        shuffle=True, stratify=y_local_enc\n",
    "    )\n",
    "\n",
    "    X_train_reg, X_test_reg, y_reg_train, y_reg_test = train_test_split(\n",
    "        X, y_reg, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"], shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Classifier Train/Test:\", X_train_cls.shape, X_test_cls.shape)\n",
    "    print(\"Regressor Train/Test:\", X_train_reg.shape, X_test_reg.shape)\n",
    "\n",
    "    # Preprocessor\n",
    "    numeric_features = [\n",
    "        'Mango_Age_Days','Days_To_Maturity','Temp_C','Humidity_%',\n",
    "        'month_sin','month_cos','doy_sin','doy_cos',\n",
    "        'local_price_lag1','local_price_roll7','local_price_roll14','local_price_mom1',\n",
    "        'export_price_lag1','export_price_roll7','export_price_roll14','export_price_mom1',\n",
    "        'price_dev','price_to_age_ratio','local_price_vol7','local_price_vol14','momentum_dev',\n",
    "        'dev_x_ratio','vol_x_mom','age_x_price','roll14_x_mom'\n",
    "    ]\n",
    "    categorical_features = ['Region','weather']\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num','passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features)\n",
    "    ], remainder='drop')\n",
    "\n",
    "    # ---------- Local Market classifier (CatBoost weighted) ----------\n",
    "    print(\"\\nTraining Local Market classifier (CatBoost weighted)...\")\n",
    "    cat_params = config[\"CAT_CLASS_PARAMS\"]\n",
    "    cb_local = CatBoostClassifier(\n",
    "        depth=cat_params[\"depth\"],\n",
    "        iterations=cat_params[\"iterations\"],\n",
    "        learning_rate=cat_params[\"learning_rate\"],\n",
    "        l2_leaf_reg=cat_params[\"l2_leaf_reg\"],\n",
    "        random_state=config[\"RANDOM_STATE\"],\n",
    "        verbose=0,\n",
    "        loss_function='MultiClass',\n",
    "        class_weights={0:1.0, 1:1.0, 2:config[\"BEST_PETTAH_WEIGHT\"]}\n",
    "    )\n",
    "    pipe_local = Pipeline([('pre', preprocessor), ('clf', cb_local)])\n",
    "    pipe_local.fit(X_train_cls, y_local_train)\n",
    "\n",
    "    # evaluate local\n",
    "    y_local_pred = pipe_local.predict(X_test_cls)\n",
    "    acc_local = accuracy_score(y_local_test, y_local_pred)\n",
    "    print(\"Local Accuracy:\", acc_local)\n",
    "    creport_local = classification_report(y_local_test, y_local_pred, target_names=le_local.classes_)\n",
    "\n",
    "    # save confusion matrix + report\n",
    "    cm_local = confusion_matrix(y_local_test, y_local_pred)\n",
    "    save_confusion_matrix(cm_local, le_local.classes_, os.path.join(CONFIG[\"PLOTS_DIR\"], \"confusion_local.png\"), title=\"Local Market Confusion Matrix\")\n",
    "    with open(os.path.join(CONFIG[\"ARTIFACT_DIR\"], \"report_local.txt\"), \"w\") as f:\n",
    "        f.write(creport_local)\n",
    "\n",
    "    # persist local model + encoders\n",
    "    joblib.dump(pipe_local, os.path.join(CONFIG[\"MODEL_DIR\"], \"local_catboost_weighted.joblib\"))\n",
    "    joblib.dump(le_local, os.path.join(CONFIG[\"MODEL_DIR\"], \"label_encoder_local.joblib\"))\n",
    "    print(\"Saved Local model and encoder to:\", CONFIG[\"MODEL_DIR\"])\n",
    "\n",
    "    # feature importances (CatBoost)\n",
    "    try:\n",
    "        cat_core = pipe_local.named_steps['clf']\n",
    "        fi = save_feature_importance_catboost(\n",
    "            cat_core, preprocessor, numeric_features, categorical_features,\n",
    "            outpath_csv=os.path.join(CONFIG[\"ARTIFACT_DIR\"], \"local_feature_importances_top20.csv\"),\n",
    "            outpath_png=os.path.join(CONFIG[\"PLOTS_DIR\"], \"feature_importances_local_top20.png\"),\n",
    "            top_n=20\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Feature importance extraction failed:\", e)\n",
    "\n",
    "    # ---------- Export Market classifier (XGBoost) ----------\n",
    "    print(\"\\nTraining Export Market classifier (XGBoost)...\")\n",
    "    xgbc_params = config[\"XGB_CLASS_PARAMS\"]\n",
    "    xgb_export = Pipeline([('pre', preprocessor), ('clf', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        n_estimators=xgbc_params['n_estimators'],\n",
    "        max_depth=xgbc_params['max_depth'],\n",
    "        learning_rate=xgbc_params['learning_rate'],\n",
    "        subsample=xgbc_params['subsample'],\n",
    "        colsample_bytree=xgbc_params['colsample_bytree'],\n",
    "        random_state=config[\"RANDOM_STATE\"],\n",
    "        n_jobs=-1\n",
    "    ))])\n",
    "    xgb_export.fit(X_train_cls, y_export_train)\n",
    "\n",
    "    # evaluate export\n",
    "    y_export_pred = xgb_export.predict(X_test_cls)\n",
    "    acc_export = accuracy_score(y_export_test, y_export_pred)\n",
    "    print(\"Export Accuracy:\", acc_export)\n",
    "    creport_export = classification_report(y_export_test, y_export_pred, target_names=le_export.classes_ if 'le_export' in locals() else np.unique(y_export))\n",
    "    cm_export = confusion_matrix(y_export_test, y_export_pred)\n",
    "    save_confusion_matrix(cm_export, (le_export.classes_ if 'le_export' in locals() else np.unique(y_export)), os.path.join(CONFIG[\"PLOTS_DIR\"], \"confusion_export.png\"), title=\"Export Market Confusion Matrix\")\n",
    "    with open(os.path.join(CONFIG[\"ARTIFACT_DIR\"], \"report_export.txt\"), \"w\") as f:\n",
    "        f.write(creport_export)\n",
    "\n",
    "    joblib.dump(xgb_export, os.path.join(CONFIG[\"MODEL_DIR\"], \"export_xgboost.joblib\"))\n",
    "    joblib.dump(le_export, os.path.join(CONFIG[\"MODEL_DIR\"], \"label_encoder_export.joblib\"))\n",
    "    print(\"Saved Export model and encoder to:\", CONFIG[\"MODEL_DIR\"])\n",
    "\n",
    "    # ---------- Multi-output regressors ----------\n",
    "    print(\"\\nTraining multi-output regressors (XGBoost & CatBoost)...\")\n",
    "    # XGBoost regressor (wrapped)\n",
    "    xgbr_params = config[\"XGB_REG_PARAMS\"]\n",
    "    xgb_reg_base = XGBRegressor(\n",
    "        n_estimators=xgbr_params['n_estimators'],\n",
    "        max_depth=xgbr_params['max_depth'],\n",
    "        learning_rate=xgbr_params['learning_rate'],\n",
    "        subsample=xgbr_params['subsample'],\n",
    "        colsample_bytree=xgbr_params['colsample_bytree'],\n",
    "        random_state=config[\"RANDOM_STATE\"],\n",
    "        n_jobs=1\n",
    "    )\n",
    "    xgb_reg = Pipeline([('pre', preprocessor), ('reg', MultiOutputRegressor(xgb_reg_base))])\n",
    "    xgb_reg.fit(X_train_reg, y_reg_train)\n",
    "\n",
    "    catr_params = config[\"CAT_REG_PARAMS\"]\n",
    "    cat_reg_base = CatBoostRegressor(\n",
    "        depth=catr_params['depth'],\n",
    "        iterations=catr_params['iterations'],\n",
    "        learning_rate=catr_params['learning_rate'],\n",
    "        l2_leaf_reg=catr_params['l2_leaf_reg'],\n",
    "        random_state=config[\"RANDOM_STATE\"],\n",
    "        verbose=0\n",
    "    )\n",
    "    cat_reg = Pipeline([('pre', preprocessor), ('reg', MultiOutputRegressor(cat_reg_base))])\n",
    "    cat_reg.fit(X_train_reg, y_reg_train)\n",
    "\n",
    "    # evaluation helper\n",
    "    def eval_and_plot_reg(name, pipeline, X_test, y_test, prefix):\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        n_targets = y_test.shape[1]\n",
    "        summary = {}\n",
    "        for i in range(n_targets):\n",
    "            rmse = mean_squared_error(y_test[:,i], y_pred[:,i], squared=False)\n",
    "            mae = mean_absolute_error(y_test[:,i], y_pred[:,i])\n",
    "            r2 = r2_score(y_test[:,i], y_pred[:,i])\n",
    "            summary[f\"target_{i}\"] = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "            # scatter\n",
    "            plt.figure(figsize=(5,4))\n",
    "            plt.scatter(y_test[:,i], y_pred[:,i], alpha=0.4, s=10)\n",
    "            mn = min(y_test[:,i].min(), y_pred[:,i].min()); mx = max(y_test[:,i].max(), y_pred[:,i].max())\n",
    "            plt.plot([mn,mx],[mn,mx],'k--')\n",
    "            plt.xlabel(\"True\"); plt.ylabel(\"Pred\")\n",
    "            plt.title(f\"{name} target_{i} (RMSE={rmse:.3f})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(CONFIG[\"PLOTS_DIR\"], f\"{prefix}_reg_target{i}_scatter.png\"))\n",
    "            plt.close()\n",
    "        return summary\n",
    "\n",
    "    xgb_reg_summary = eval_and_plot_reg(\"XGB_Regressor\", xgb_reg, X_test_reg, y_reg_test, \"xgb\")\n",
    "    cat_reg_summary = eval_and_plot_reg(\"Cat_Regressor\", cat_reg, X_test_reg, y_reg_test, \"cat\")\n",
    "    print(\"XGB reg summary:\", xgb_reg_summary)\n",
    "    print(\"Cat reg summary:\", cat_reg_summary)\n",
    "\n",
    "    joblib.dump(xgb_reg, os.path.join(CONFIG[\"MODEL_DIR\"], \"regressor_xgb_multi.joblib\"))\n",
    "    joblib.dump(cat_reg, os.path.join(CONFIG[\"MODEL_DIR\"], \"regressor_cat_multi.joblib\"))\n",
    "    print(\"Saved regressors to:\", CONFIG[\"MODEL_DIR\"])\n",
    "\n",
    "    # ---------- Save training summary & metadata ----------\n",
    "    summary = {\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"local_accuracy\": float(acc_local),\n",
    "        \"export_accuracy\": float(acc_export),\n",
    "        \"xgb_reg_summary\": xgb_reg_summary,\n",
    "        \"cat_reg_summary\": cat_reg_summary,\n",
    "        \"models_saved\": {\n",
    "            \"local\": os.path.join(CONFIG[\"MODEL_DIR\"], \"local_catboost_weighted.joblib\"),\n",
    "            \"export\": os.path.join(CONFIG[\"MODEL_DIR\"], \"export_xgboost.joblib\"),\n",
    "            \"xgb_reg\": os.path.join(CONFIG[\"MODEL_DIR\"], \"regressor_xgb_multi.joblib\"),\n",
    "            \"cat_reg\": os.path.join(CONFIG[\"MODEL_DIR\"], \"regressor_cat_multi.joblib\"),\n",
    "            \"le_local\": os.path.join(CONFIG[\"MODEL_DIR\"], \"label_encoder_local.joblib\"),\n",
    "            \"le_export\": os.path.join(CONFIG[\"MODEL_DIR\"], \"label_encoder_export.joblib\")\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(CONFIG[\"ARTIFACT_DIR\"], \"training_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # also write human-readable summary\n",
    "    with open(os.path.join(CONFIG[\"ARTIFACT_DIR\"], \"training_summary.txt\"), \"w\") as f:\n",
    "        f.write(\"Local Accuracy: %.6f\\n\\n\" % acc_local)\n",
    "        f.write(\"Local Classification Report:\\n\")\n",
    "        f.write(creport_local + \"\\n\\n\")\n",
    "        f.write(\"Export Accuracy: %.6f\\n\\n\" % acc_export)\n",
    "        f.write(\"Export Classification Report:\\n\")\n",
    "        f.write(creport_export + \"\\n\\n\")\n",
    "        f.write(\"XGB Reg Summary:\\n\"); f.write(str(xgb_reg_summary) + \"\\n\\n\")\n",
    "        f.write(\"Cat Reg Summary:\\n\"); f.write(str(cat_reg_summary) + \"\\n\\n\")\n",
    "    print(\"Training artifacts and summary saved under:\", CONFIG[\"ARTIFACT_DIR\"])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135fd86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (12000, 12)\n",
      "Prepared features; columns: 39\n",
      "Classifier Train/Test: (9600, 27) (2400, 27)\n",
      "Regressor Train/Test: (9600, 27) (2400, 27)\n",
      "\n",
      "Training Local Market classifier (CatBoost weighted)...\n",
      "Local Accuracy: 0.6625\n",
      "Saved Local model and encoder to: output\\models\n",
      "\n",
      "Training Export Market classifier (XGBoost)...\n",
      "Export Accuracy: 0.6341666666666667\n",
      "Saved Export model and encoder to: output\\models\n",
      "\n",
      "Training multi-output regressors (XGBoost & CatBoost)...\n",
      "XGB reg summary: {'target_0': {'rmse': 3.355285689206157, 'mae': 2.543561069488525, 'r2': 0.996905558402411}, 'target_1': {'rmse': 0.021986383960749194, 'mae': 0.015838696230053902, 'r2': 0.9944478753949237}, 'target_2': {'rmse': 13.486434440791902, 'mae': 10.489394558461507, 'r2': 0.9620814529146919}}\n",
      "Cat reg summary: {'target_0': {'rmse': 1.984412312491834, 'mae': 1.4958844840631613, 'r2': 0.9989176016851002}, 'target_1': {'rmse': 0.013014207330710462, 'mae': 0.008179379915632905, 'r2': 0.9980546980113943}, 'target_2': {'rmse': 13.004523624878955, 'mae': 10.205985836452959, 'r2': 0.9647429238996625}}\n",
      "Saved regressors to: output\\models\n",
      "Training artifacts and summary saved under: output\\artifacts\n",
      "\n",
      "Done. See output folders:\n",
      " - models: output\\models\n",
      " - artifacts: output\\artifacts\n",
      " - plots: output\\artifacts\\plots\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    cfg = CONFIG.copy()\n",
    "    summary = train_and_evaluate(cfg[\"CSV_PATH\"], cfg)\n",
    "    print(\"\\nDone. See output folders:\")\n",
    "    print(\" - models:\", cfg[\"MODEL_DIR\"])\n",
    "    print(\" - artifacts:\", cfg[\"ARTIFACT_DIR\"])\n",
    "    print(\" - plots:\", cfg[\"PLOTS_DIR\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b4909",
   "metadata": {},
   "source": [
    "#### INFERANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c63dad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "\n",
      "Predictions:\n",
      "      Date     Region Local_Market  Local_Price_LKR Export_Market  Export_Price_USD  Harvesting_after_3months_price\n",
      "2025-11-15    Colombo      Colombo       384.316063         Dubai          1.360861                      414.158614\n",
      "2025-01-10 Hambantota      Colombo       421.674971         Paris          2.060027                      483.117209\n",
      "\n",
      "JSON results:\n",
      "[\n",
      "  {\n",
      "    \"input\": {\n",
      "      \"Date\": \"2025-11-15\",\n",
      "      \"Mango_Age_Days\": 40,\n",
      "      \"Days_To_Maturity\": 40,\n",
      "      \"Temp_C\": 30.2,\n",
      "      \"Humidity_%\": 72,\n",
      "      \"Region\": \"Colombo\",\n",
      "      \"weather\": \"Clear\"\n",
      "    },\n",
      "    \"Local_Market\": \"Colombo\",\n",
      "    \"Local_Market_proba\": [\n",
      "      0.9981093464095814,\n",
      "      3.1291909487282736e-05,\n",
      "      0.0018593616809314231\n",
      "    ],\n",
      "    \"Export_Market\": \"Dubai\",\n",
      "    \"Export_Market_proba\": [\n",
      "      0.08683250844478607,\n",
      "      0.6086015105247498,\n",
      "      0.13983270525932312,\n",
      "      0.16473327577114105\n",
      "    ],\n",
      "    \"Predicted_Local_Price_LKR\": 384.31606265034884,\n",
      "    \"Predicted_Export_Price_USD\": 1.360860510927478,\n",
      "    \"Predicted_Harvesting_after_3months_price\": 414.1586142366974\n",
      "  },\n",
      "  {\n",
      "    \"input\": {\n",
      "      \"Date\": \"2025-01-10\",\n",
      "      \"Mango_Age_Days\": 120,\n",
      "      \"Days_To_Maturity\": 5,\n",
      "      \"Temp_C\": 27.5,\n",
      "      \"Humidity_%\": 78,\n",
      "      \"Region\": \"Hambantota\",\n",
      "      \"weather\": \"Rain\"\n",
      "    },\n",
      "    \"Local_Market\": \"Colombo\",\n",
      "    \"Local_Market_proba\": [\n",
      "      0.699320743775506,\n",
      "      0.019545776317478735,\n",
      "      0.28113347990701526\n",
      "    ],\n",
      "    \"Export_Market\": \"Paris\",\n",
      "    \"Export_Market_proba\": [\n",
      "      0.003189236856997013,\n",
      "      0.0171639584004879,\n",
      "      0.3458423614501953,\n",
      "      0.633804440498352\n",
      "    ],\n",
      "    \"Predicted_Local_Price_LKR\": 421.6749707369321,\n",
      "    \"Predicted_Export_Price_USD\": 2.0600273048865767,\n",
      "    \"Predicted_Harvesting_after_3months_price\": 483.11720909549683\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# predict_batch_from_json.py\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------\n",
    "# User inputs (sample)\n",
    "# -------------------------\n",
    "sample_inputs = [\n",
    "    {\n",
    "        \"Date\": \"2025-11-15\",\n",
    "        \"Mango_Age_Days\": 40,\n",
    "        \"Days_To_Maturity\": 40,\n",
    "        \"Temp_C\": 30.2,\n",
    "        \"Humidity_%\": 72,\n",
    "        \"Region\": \"Colombo\",\n",
    "        \"weather\": \"Clear\"\n",
    "    },\n",
    "    {\n",
    "        \"Date\": \"2025-01-10\",\n",
    "        \"Mango_Age_Days\": 120,\n",
    "        \"Days_To_Maturity\": 5,\n",
    "        \"Temp_C\": 27.5,\n",
    "        \"Humidity_%\": 78,\n",
    "        \"Region\": \"Hambantota\",\n",
    "        \"weather\": \"Rain\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Paths (try both conventions)\n",
    "# -------------------------\n",
    "possible_model_dirs = [\"model\", \"output/models\", \"output/model\", \"models\"]\n",
    "def find_path(rel):\n",
    "    for base in possible_model_dirs:\n",
    "        p = os.path.join(base, rel)\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    # fallback to direct path\n",
    "    if os.path.exists(rel):\n",
    "        return rel\n",
    "    return None\n",
    "\n",
    "LOCAL_MODEL_PATH = find_path(\"local_catboost_weighted.joblib\")\n",
    "EXPORT_MODEL_PATH = find_path(\"export_xgboost.joblib\")\n",
    "REG_MODEL_PATH = find_path(\"regressor_cat_multi.joblib\")\n",
    "LE_LOCAL_PATH = find_path(\"label_encoder_local.joblib\")\n",
    "LE_EXPORT_PATH = find_path(\"label_encoder_export.joblib\")\n",
    "CSV_PATH = \"dataset/mango_price_dataset.csv\"\n",
    "\n",
    "# check\n",
    "for p,name in [(LOCAL_MODEL_PATH,\"Local model\"), (EXPORT_MODEL_PATH,\"Export model\"), (REG_MODEL_PATH,\"Regressor\"), (LE_LOCAL_PATH,\"LE Local\"), (LE_EXPORT_PATH,\"LE Export\"), (CSV_PATH,\"History CSV\")]:\n",
    "    if p is None or (isinstance(p,str) and not os.path.exists(p)):\n",
    "        if name in (\"LE Local\",\"LE Export\"):\n",
    "            # label encoders optional if pipeline outputs labels directly\n",
    "            print(f\"Warning: {name} not found at expected places. If model pipelines return labels directly that's okay.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Required artifact not found: {name}. Tried several locations. Looking for files like: {p}\")\n",
    "\n",
    "# -------------------------\n",
    "# Load models & encoders\n",
    "# -------------------------\n",
    "print(\"Loading models...\")\n",
    "local_pipe = joblib.load(LOCAL_MODEL_PATH)\n",
    "export_pipe = joblib.load(EXPORT_MODEL_PATH)\n",
    "reg_pipe = joblib.load(REG_MODEL_PATH)\n",
    "\n",
    "le_local = joblib.load(LE_LOCAL_PATH) if LE_LOCAL_PATH and os.path.exists(LE_LOCAL_PATH) else None\n",
    "le_export = joblib.load(LE_EXPORT_PATH) if LE_EXPORT_PATH and os.path.exists(LE_EXPORT_PATH) else None\n",
    "\n",
    "# -------------------------\n",
    "# Feature engineering function (same as training)\n",
    "# -------------------------\n",
    "def prepare_full_features(df):\n",
    "    d = df.copy()\n",
    "    d['Date'] = pd.to_datetime(d['Date'])\n",
    "    d = d.sort_values(['Region', 'Date']).reset_index(drop=True)\n",
    "    # time features\n",
    "    d['day'] = d['Date'].dt.day\n",
    "    d['month'] = d['Date'].dt.month\n",
    "    d['dayofyear'] = d['Date'].dt.dayofyear\n",
    "    d['year'] = d['Date'].dt.year\n",
    "    d['month_sin'] = np.sin(2 * np.pi * d['month'] / 12)\n",
    "    d['month_cos'] = np.cos(2 * np.pi * d['month'] / 12)\n",
    "    d['doy_sin'] = np.sin(2 * np.pi * d['dayofyear'] / 365)\n",
    "    d['doy_cos'] = np.cos(2 * np.pi * d['dayofyear'] / 365)\n",
    "\n",
    "    # lags\n",
    "    d['local_price_lag1'] = d.groupby('Region')['Local_Price_LKR'].shift(1)\n",
    "    d['export_price_lag1'] = d.groupby('Region')['Export_Price_USD'].shift(1)\n",
    "\n",
    "    # rolling means\n",
    "    d['local_price_roll7'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(7, min_periods=1).mean())\n",
    "    d['local_price_roll14'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
    "    d['export_price_roll7'] = d.groupby('Region')['Export_Price_USD'].transform(lambda s: s.rolling(7, min_periods=1).mean())\n",
    "    d['export_price_roll14'] = d.groupby('Region')['Export_Price_USD'].transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
    "\n",
    "    # momentum\n",
    "    d['local_price_mom1'] = d['Local_Price_LKR'] - d['local_price_lag1']\n",
    "    d['export_price_mom1'] = d['Export_Price_USD'] - d['export_price_lag1']\n",
    "\n",
    "    # fills for lag/rolling\n",
    "    grp_med_local = d.groupby('Region')['Local_Price_LKR'].transform('median')\n",
    "    grp_med_export = d.groupby('Region')['Export_Price_USD'].transform('median')\n",
    "\n",
    "    for col in ['local_price_lag1','local_price_roll7','local_price_roll14']:\n",
    "        d[col] = d[col].fillna(grp_med_local).fillna(d['Local_Price_LKR'].median())\n",
    "    for col in ['export_price_lag1','export_price_roll7','export_price_roll14']:\n",
    "        d[col] = d[col].fillna(grp_med_export).fillna(d['Export_Price_USD'].median())\n",
    "    d['local_price_mom1'] = d['local_price_mom1'].fillna(0)\n",
    "    d['export_price_mom1'] = d['export_price_mom1'].fillna(0)\n",
    "\n",
    "    # new features\n",
    "    d['local_price_region_mean'] = d.groupby('Region')['Local_Price_LKR'].transform('mean')\n",
    "    d['price_dev'] = d['Local_Price_LKR'] - d['local_price_region_mean']\n",
    "    d['price_to_age_ratio'] = d['Local_Price_LKR'] / (d['Mango_Age_Days'].replace(0,1))\n",
    "    d['local_price_vol7'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(7, min_periods=1).std()).fillna(0)\n",
    "    d['local_price_vol14'] = d.groupby('Region')['Local_Price_LKR'].transform(lambda s: s.rolling(14, min_periods=1).std()).fillna(0)\n",
    "    d['local_mom_region_mean'] = d.groupby('Region')['local_price_mom1'].transform('mean')\n",
    "    d['momentum_dev'] = d['local_price_mom1'] - d['local_mom_region_mean']\n",
    "\n",
    "    # interactions\n",
    "    d['dev_x_ratio'] = d['price_dev'] * d['price_to_age_ratio']\n",
    "    d['vol_x_mom'] = d['local_price_vol14'] * d['momentum_dev']\n",
    "    d['age_x_price'] = d['Mango_Age_Days'] * d['local_price_lag1']\n",
    "    d['roll14_x_mom'] = d['local_price_roll14'] * d['local_price_mom1']\n",
    "\n",
    "    # fill residual NaNs\n",
    "    fill_cols = ['price_dev','price_to_age_ratio','local_price_vol7','local_price_vol14','momentum_dev','dev_x_ratio','vol_x_mom','age_x_price','roll14_x_mom']\n",
    "    for c in fill_cols:\n",
    "        if c in d.columns:\n",
    "            d[c] = d[c].fillna(0)\n",
    "    return d\n",
    "\n",
    "# -------------------------\n",
    "# Load historical dataset\n",
    "# -------------------------\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"History CSV not found at '{CSV_PATH}'. It is required to compute lag/rolling features.\")\n",
    "\n",
    "hist = pd.read_csv(CSV_PATH)\n",
    "hist['Date'] = pd.to_datetime(hist['Date'])\n",
    "# ensure required price columns exist\n",
    "for col in ['Local_Price_LKR','Export_Price_USD','Harvesting_after_3months_price']:\n",
    "    if col not in hist.columns:\n",
    "        hist[col] = np.nan\n",
    "\n",
    "# -------------------------\n",
    "# Build DataFrame of new inputs + dummy price columns for lagging\n",
    "# -------------------------\n",
    "new_rows = []\n",
    "for inp in sample_inputs:\n",
    "    row = inp.copy()\n",
    "    row['Date'] = pd.to_datetime(row['Date'])\n",
    "    # For lag/rolling logic we need a Local/Export/Harvesting price in new row;\n",
    "    # use latest available from same Region in history (if exists), else region median, else global median.\n",
    "    region = row['Region']\n",
    "    region_hist = hist[hist['Region'] == region]\n",
    "    if len(region_hist) > 0:\n",
    "        last_local = float(region_hist.sort_values('Date').iloc[-1]['Local_Price_LKR'])\n",
    "        last_export = float(region_hist.sort_values('Date').iloc[-1]['Export_Price_USD'])\n",
    "        last_harv = float(region_hist.sort_values('Date').iloc[-1]['Harvesting_after_3months_price'])\n",
    "    else:\n",
    "        # fallbacks\n",
    "        last_local = float(hist['Local_Price_LKR'].median())\n",
    "        last_export = float(hist['Export_Price_USD'].median())\n",
    "        last_harv = float(hist['Harvesting_after_3months_price'].median())\n",
    "    row['Local_Price_LKR'] = last_local\n",
    "    row['Export_Price_USD'] = last_export\n",
    "    row['Harvesting_after_3months_price'] = last_harv\n",
    "    new_rows.append(row)\n",
    "\n",
    "df_new = pd.DataFrame(new_rows)\n",
    "\n",
    "# -------------------------\n",
    "# Concatenate hist + new, compute features, then keep only new final rows\n",
    "# -------------------------\n",
    "combined = pd.concat([hist, df_new], ignore_index=True, sort=False)\n",
    "combined = prepare_full_features(combined)\n",
    "\n",
    "# Extract final N rows corresponding to our new inputs in the same order\n",
    "# We'll match by Date+Region+Mango_Age_Days heuristic (if duplicates exist the last occurrence will be used)\n",
    "results = []\n",
    "for inp in sample_inputs:\n",
    "    # find matching rows\n",
    "    mask = (\n",
    "        (combined['Region'] == inp['Region']) &\n",
    "        (combined['Date'] == pd.to_datetime(inp['Date'])) &\n",
    "        (combined['Mango_Age_Days'] == inp['Mango_Age_Days'])\n",
    "    )\n",
    "    match = combined[mask]\n",
    "    if match.empty:\n",
    "        # fallback: take last row for that region with that date\n",
    "        match = combined[(combined['Region']==inp['Region']) & (combined['Date']==pd.to_datetime(inp['Date']))]\n",
    "    if match.empty:\n",
    "        # as ultimate fallback, take the last row for that region\n",
    "        match = combined[combined['Region']==inp['Region']].tail(1)\n",
    "    if match.empty:\n",
    "        raise RuntimeError(\"Could not align engineered features for input: \" + str(inp))\n",
    "    sample_row = match.tail(1)  # ensure single row\n",
    "    # keep original order values\n",
    "    sample_row = sample_row.reset_index(drop=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # Make predictions\n",
    "    # -------------------------\n",
    "    # classifiers\n",
    "    local_pred_idx = local_pipe.predict(sample_row)[0]\n",
    "    try:\n",
    "        local_proba = local_pipe.predict_proba(sample_row)[0].tolist()\n",
    "    except Exception:\n",
    "        # pipeline might not expose predict_proba (unlikely), handle gracefully\n",
    "        local_proba = None\n",
    "\n",
    "    export_pred_idx = export_pipe.predict(sample_row)[0]\n",
    "    try:\n",
    "        export_proba = export_pipe.predict_proba(sample_row)[0].tolist()\n",
    "    except Exception:\n",
    "        export_proba = None\n",
    "\n",
    "    # regressors (multi-output)\n",
    "    reg_pred = reg_pipe.predict(sample_row)[0].tolist()\n",
    "\n",
    "    # decode labels if encoders available\n",
    "    local_label = le_local.inverse_transform([local_pred_idx])[0] if le_local is not None else str(local_pred_idx)\n",
    "    export_label = le_export.inverse_transform([export_pred_idx])[0] if le_export is not None else str(export_pred_idx)\n",
    "\n",
    "    out = {\n",
    "        \"input\": inp,\n",
    "        \"Local_Market\": local_label,\n",
    "        \"Local_Market_proba\": local_proba,\n",
    "        \"Export_Market\": export_label,\n",
    "        \"Export_Market_proba\": export_proba,\n",
    "        \"Predicted_Local_Price_LKR\": reg_pred[0],\n",
    "        \"Predicted_Export_Price_USD\": reg_pred[1],\n",
    "        \"Predicted_Harvesting_after_3months_price\": reg_pred[2]\n",
    "    }\n",
    "    results.append(out)\n",
    "\n",
    "# -------------------------\n",
    "# Print results (table)\n",
    "# -------------------------\n",
    "df_res = pd.DataFrame([{\n",
    "    \"Date\": r[\"input\"][\"Date\"],\n",
    "    \"Region\": r[\"input\"][\"Region\"],\n",
    "    \"Local_Market\": r[\"Local_Market\"],\n",
    "    \"Local_Price_LKR\": r[\"Predicted_Local_Price_LKR\"],\n",
    "    \"Export_Market\": r[\"Export_Market\"],\n",
    "    \"Export_Price_USD\": r[\"Predicted_Export_Price_USD\"],\n",
    "    \"Harvesting_after_3months_price\": r[\"Predicted_Harvesting_after_3months_price\"]\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(df_res.to_string(index=False))\n",
    "\n",
    "# Also print JSON\n",
    "print(\"\\nJSON results:\")\n",
    "print(json.dumps(results, indent=2, default=str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
